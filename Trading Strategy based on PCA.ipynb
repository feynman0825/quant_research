{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(\"  scikit-learn: %s\" % sklearn.__version__)\n",
    "print(\"  tensorflow: %s\" % tf.__version__)\n",
    "\n",
    "\n",
    "try:\n",
    "    import sklearn.model_selection\n",
    "    import sklearn.linear_model\n",
    "except:\n",
    "    print(\"Looks like an older version of sklearn package\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"  pandas: %s\"% pd.__version__)\n",
    "except:\n",
    "    print(\"Missing pandas package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_returns(r_df):\n",
    "    \"\"\"\n",
    "    Normalize, i.e. center and divide by standard deviation raw asset returns data\n",
    "\n",
    "    Arguments:\n",
    "    r_df -- a pandas.DataFrame of asset returns\n",
    "\n",
    "    Return:\n",
    "    normed_df -- normalized returns\n",
    "    \"\"\"\n",
    "    mean_r = r_df.mean(axis=0)\n",
    "    sd_r = r_df.std(axis=0)\n",
    "    normed_df = (r_df - mean_r) / sd_r\n",
    "    return normed_df\n",
    "\n",
    "\n",
    "def exponent_weighting(n_periods, half_life = 252):\n",
    "    \"\"\"\n",
    "    Calculate exponentially smoothed normalized (in probability density function sense) weights\n",
    "\n",
    "    Arguments:\n",
    "    n_periods -- number of periods, an integer, N in the formula above\n",
    "    half_life -- half-life, which determines the speed of decay, h in the formula\n",
    "    \n",
    "    Return:\n",
    "    exp_probs -- exponentially smoothed weights, np.array\n",
    "    \"\"\"\n",
    "    \n",
    "    exp_probs = np.zeros(n_periods) \n",
    "    X = np.array([np.exp(-np.log(2)/half_life*j) for j in range(n_periods+1)])\n",
    "    exp_probs = X/X.sum()\n",
    "    \n",
    "    return exp_probs\n",
    "\n",
    "def absorption_ratio(explained_variance, n_components):\n",
    "    \"\"\"\n",
    "    Calculate absorption ratio via PCA. absorption_ratio() is NOT to be used with Auto-Encoder. \n",
    "    \n",
    "    Arguments:\n",
    "    explained_variance -- 1D np.array of explained variance by each pricincipal component, in descending order\n",
    "    \n",
    "    n_components -- an integer, a number of principal components to compute absorption ratio\n",
    "    \n",
    "    Return:\n",
    "    ar -- absorption ratio\n",
    "    \"\"\"\n",
    "    ar = np.sum(explained_variance[:n_components]) / np.sum(explained_variance)\n",
    "    return ar\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "class LinearAutoEncoder:\n",
    "    \"\"\"\n",
    "    To perform simple PCA, we set activation_fn=None \n",
    "    i.e., all neurons are linear and the cost function is the Mean-Square Error (MSE)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_codings, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        n_outputs = n_inputs\n",
    "        self.destroy()\n",
    "        reset_graph()\n",
    "    \n",
    "        # the inputs are n_inputs x n_inputs covariance matrices\n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, n_inputs, n_inputs])\n",
    "        with tf.name_scope(\"lin_ae\"):\n",
    "            #self.codings_layer = None\n",
    "            #self.outputs = None\n",
    "            ### START CODE HERE ### (≈ 2 lines of code)\n",
    "            self.codings_layer = fully_connected(self.X, num_outputs=n_codings, activation_fn=None)\n",
    "            self.outputs = fully_connected(self.codings_layer, num_outputs=n_outputs, activation_fn=None)\n",
    "            ### END CODE HERE ###\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.reconstruction_loss = tf.reduce_mean(tf.square(self.outputs - self.X))\n",
    "            self.training_op = tf.train.AdamOptimizer(learning_rate).minimize(self.reconstruction_loss)\n",
    "            ### START CODE HERE ### (≈ 4-5 lines of code)\n",
    "            \n",
    "            ### END CODE HERE ###\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "    def destroy(self):\n",
    "        if hasattr(self, 'sess') and self.sess is not None:\n",
    "            self.sess.close()\n",
    "            self.sess = None\n",
    "\n",
    "    def absorption_ratio(self, test_input):\n",
    "        \"\"\"\n",
    "        Calculate absorption ratio based on already trained model\n",
    "        \"\"\"\n",
    "        if self.outputs is None:\n",
    "            return test_input, 0.\n",
    "        \n",
    "        with self.sess.as_default():  # do not close session\n",
    "            codings = self.codings_layer.eval(feed_dict={self.X: test_input})\n",
    "\n",
    "            # calculate variance explained ratio\n",
    "            result_ = self.outputs.eval(feed_dict={self.X: test_input})\n",
    "            var_explained = np.sum(np.diag(result_.squeeze())) / np.sum(np.diag(test_input.squeeze()))\n",
    "\n",
    "        return codings[0, :, :], var_explained\n",
    "    \n",
    "    def next_batch(self, X_train, batch_size):\n",
    "        \"\"\"\n",
    "        X_train - np.array of double of size K x N x N, where N is dimensionality of the covariance matrix\n",
    "        batch_size - an integer, number of training examples to feed through the nwtwork at once\n",
    "        \"\"\"\n",
    "        y_batch = None\n",
    "\n",
    "        selected_idx = np.random.choice(tuple(range(X_train.shape[0])), size=batch_size)\n",
    "        X_batch = X_train[selected_idx, :, :]\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def train(self, X_train, X_test, n_epochs=5, batch_size=2, verbose=False):\n",
    "        \"\"\"\n",
    "        train simple auto-encoder network\n",
    "        :param X_train:\n",
    "        :param X_test:\n",
    "        :param n_epochs: number of epochs to use for training the model\n",
    "        :param batch_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.outputs is None:\n",
    "            return X_test, 0.\n",
    "        \n",
    "        n_examples = len(X_train)  # number of training examples\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # as_default context manager does not close the session when you exit the context,\n",
    "        # and you must close the session explicitly.\n",
    "        with self.sess.as_default():\n",
    "            self.init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                n_batches = n_examples // min(n_examples, batch_size)\n",
    "                for _ in range(n_batches):\n",
    "                    X_batch, y_batch = self.next_batch(X_train, batch_size)\n",
    "                    self.sess.run(self.training_op, feed_dict={self.X: X_batch})\n",
    "                \n",
    "                if verbose:\n",
    "                    # last covariance matrix from the training sample\n",
    "                    if X_train.shape[0] == 1:\n",
    "                        mse_train = self.reconstruction_loss.eval(feed_dict={self.X: X_train})\n",
    "                    else:\n",
    "                        mse_train = self.reconstruction_loss.eval(feed_dict={self.X: np.array([X_train[-1, :, :]])})\n",
    "                    mse_test = self.reconstruction_loss.eval(feed_dict={self.X: X_test})\n",
    "                    print('Epoch %d. MSE Train %.4f, MSE Test %.4f' % (epoch, mse_train, mse_test))\n",
    "\n",
    "            # calculate variance explained ratio\n",
    "            test_input = np.array([X_train[-1, :, :]])\n",
    "            result_ = self.outputs.eval(feed_dict={self.X: test_input})\n",
    "            var_explained = np.sum(np.diag(result_.squeeze())) / np.sum(np.diag(test_input.squeeze()))\n",
    "            # print('Linear Auto-Encoder: variance explained: %.2f' % var_explained)\n",
    "\n",
    "            codings = self.codings_layer.eval(feed_dict={self.X: X_test})\n",
    "            # print('Done training linear auto-encoder')\n",
    "\n",
    "        return codings[0, :, :], var_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
